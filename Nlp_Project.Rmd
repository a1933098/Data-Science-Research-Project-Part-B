---
title: "Nlp_Project"
output: html_document
date: "2025-11-19"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This file downloads the book 'Around the World in 80 Days’ from Project Gutenberg, does basic cleaning of the book, and saves the data in a csv file

##### Author: Ann Mary Anish
##### Last Modified: November 2025


## 1. LOAD LIBRARIES

```{r}


pacman::p_load(tidyverse, textdata, tidytext, syuzhet, gutenbergr, textstem,
               stringr, ggrepel, dplyr, ggplot2, lexicon, grid, zoo, spacyr)

```
## 2. DOWNLOAD BOOK FROM PROJECT GUTENBERG

```{r}

library(gutenbergr)

book <- gutenberg_download(103, mirror = "https://www.gutenberg.org")

```
## 3. CLEAN TEXT AND SEPARATE INTO CHAPTERS
```{r}

#Remove Header and Footer 
book_text_only <- book %>%
  tail(nrow(book) - 53)  # Removes top header lines

## Separate Book by Chapters 
book_chapters <- book_text_only %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))
  ) %>%
  fill(chapter, .direction = "down") %>%
  ungroup()

## Remove Empty Lines 
book_chapters_clean <- book_chapters %>%
  filter(text != "")

## Remove Chapter Headings 
ch <- 0
rows <- c()
for (i in 1:nrow(book_chapters_clean)) {
  temp_ch <- book_chapters_clean[i, "chapter"]
  if (!(temp_ch == ch)) {
    ch <- temp_ch
    rows <- c(rows, i)
  } else {
    if (str_detect(book_chapters_clean[i, "text"][[1]],
                   "^([[:upper:]]|[[:digit:]]|[[:punct:]]|[[:blank:]])+$")) {
      rows <- c(rows, i)
    }
  }
}
rows <- unique(rows)

# Remove those rows
book_chapters_clean2 <- book_chapters_clean %>%
  filter(!row_number() %in% rows)

## Combine Text by Chapter 
book_chapters_clean3 <- book_chapters_clean2 %>%
  select(chapter, text) %>%
  group_by(chapter) %>%
  summarise(text = paste(text, collapse = " "), .groups = "drop")

```

# 4. SAVE CLEANED CHAPTER DATA
```{r}

write.csv(book_chapters_clean3, "around_world_80_days.csv", row.names = FALSE)
```

# 5. TOKENISATION AND TEXT CLEANING
```{r}

data("stop_words")

book_tokens <- book_chapters_clean3 %>%
  mutate(text = str_to_lower(text),                              # Lowercase
         text = str_replace_all(text, "[[:punct:]]", " "),       # Remove punctuation
         text = str_replace_all(text, "[[:digit:]]", ""),        # Remove numbers
         text = str_replace_all(text, "\\s+", " ")) %>%          # Remove extra whitespace
  unnest_tokens(word, text) %>%                                  # Tokenisation
  anti_join(stop_words, by = "word") %>%                         # Remove stop words
  mutate(word = lemmatize_words(word))                           # Lemmatise

```

# 6. SAVE TOKENISED DATA
```{r}
## Save Preprocessed Tokens 
write.csv(book_tokens, "tokens_for_sentiment.csv", row.names = FALSE)

```

# 7. LOAD PREPROCESSED TOKENS AND COUNT UNIQUE WORDS
```{r}

tokens <- read.csv("tokens_for_sentiment.csv")
unique_words <- length(unique(book_tokens$word))

```

# 8. LOAD & PREPARE SENTIMENT LEXICONS
```{r}

# --- AFINN ---
afinn <- get_sentiments("afinn") %>%
  mutate(word = tolower(word))

# --- Bing ---
bing <- get_sentiments("bing") %>%
  mutate(word = tolower(word))

# --- NRC (only positive and negative) ---
nrc <- get_sentiments("nrc") %>%
  filter(sentiment %in% c("positive", "negative")) %>%
  distinct(word, sentiment, .keep_all = TRUE) %>%
  mutate(word = tolower(word))

# --- SenticNet ---
senticnet_lex <- lexicon::hash_sentiment_senticnet %>%
  transmute(word = tolower(x), sentiment = as.numeric(y))

# --- SentiWordNet ---
sentiword_lex <- lexicon::hash_sentiment_sentiword %>%
  transmute(word = tolower(x), sentiment = as.numeric(y))

```

# 9. CALCULATE LEXICON COVERAGE
```{r}

# Function to calculate coverage for a given lexicon
calc_coverage <- function(data, lexicon) {
  data %>%
    inner_join(lexicon, by = "word") %>%
    summarise(matched_words = n_distinct(word)) %>%
    mutate(coverage = matched_words / unique_words * 100)
}

# Coverage for each lexicon
afinn_coverage       <- calc_coverage(book_tokens, afinn)
bing_coverage        <- calc_coverage(book_tokens, bing)
nrc_coverage         <- calc_coverage(book_tokens, nrc)
sentiword_coverage   <- calc_coverage(book_tokens, sentiword_lex)
senticnet_coverage   <- calc_coverage(book_tokens, senticnet_lex)

# Combine all coverage results into a single dataframe
coverage_df <- bind_rows(
  AFINN        = afinn_coverage,
  Bing         = bing_coverage,
  NRC          = nrc_coverage,
  SentiWordNet = sentiword_coverage,
  SenticNet    = senticnet_coverage,
  .id = "Lexicon"
) %>%
  arrange(desc(coverage))

# View results
print(coverage_df)

# Save coverage data
write.csv(coverage_df, "sentiment_lexicon_coverage.csv", row.names = FALSE)

```

# 10. CALCULATE SENTICNET SENTIMENT PER CHAPTER
```{r}

#  SenticNet Sentiment per chapter (Highest coverage)
sentic_sentiment <- book_tokens %>%
  inner_join(senticnet_lex, by = "word") %>%
  group_by(chapter) %>%
  summarise(sentiment_score = mean(sentiment, na.rm = TRUE)) %>%
  mutate(method = "SenticNet")


# Rolling mean smoothing 
sentic_sentiment <- sentic_sentiment %>%
  mutate(rolling_sentiment = rollmean(sentiment_score, k = 1, fill = NA, align = "center"))

print(sentic_sentiment)
```

# 11. PLOT EMOTIONAL ARC
```{r}

# Figure 1
ggplot(sentic_sentiment, aes(x = as.numeric(chapter))) +
  geom_point(aes(y = sentiment_score), alpha = 0.5, color = "grey50") +
  geom_line(aes(y = rolling_sentiment), color = "red", size = 1.2) +
  labs(
    title = "Figure 1. Emotional Arc of 'Around the World in 80 Days'",
    subtitle = "Sentiment per chapter (SenticNet, rolling mean smoothing)",
    x = "Chapter",
    y = "Average Sentiment Score"
  ) +
  theme_minimal(base_size = 16) +
  theme(
    plot.title = element_text(face = "bold",size = 15),
    plot.subtitle = element_text(size = 13)
  )

```

# 12. ADD KEY JOURNEY EVENTS TO THE EMOTIONAL ARC
```{r}
# Figure 2
events <- data.frame(
  chapter = c(2, 4, 9, 14, 19, 24, 30, 37),
  rolling_sentiment = c(0.12, 0.10, 0.12, 0.12, 0.00, 0.10, -0.02, 0.20),
  label = LETTERS[1:8],
  description = c(
    "Fogg departs London",
    "Reaches Suez",
    "Arrives in Bombay",
    "Saves Aouda",
    "Storm in China Sea",
    "Rescues Passepartout",
    "Arrest in Liverpool",
    "Wins the wager"
  )
)

ggplot() +
  # Emotional arc line
  geom_line(data = sentic_sentiment,
            aes(x = chapter, y = rolling_sentiment),
            color = "red", size = 1.3) +
  
  # Yellow event points
  geom_point(data = events,
             aes(x = chapter, y = rolling_sentiment),
             color = "black", fill = "yellow", size = 4, shape = 21) +
  
  # Labels (A–H)
  geom_text(data = events,
            aes(x = chapter, y = rolling_sentiment, label = label),
            vjust = -1, size = 5, fontface = "bold") +
  
  # Descriptions on the right (compact spacing)
  geom_text(data = events,
            aes(x = max(sentic_sentiment$chapter) + 1.5,
                y = rev(seq(min(sentic_sentiment$rolling_sentiment, na.rm = TRUE),
                            max(sentic_sentiment$rolling_sentiment, na.rm = TRUE),
                            length.out = nrow(events))),
                label = paste0(label, ": ", description)),
            hjust = 0, size = 3.8) +
  
  # Labels & theme
  labs(
    title = "Figure 2. Emotional Arc of 'Around the World in 80 Days'",
    subtitle = "Sentiment per chapter using SenticNet (rolling mean)",
    x = "Chapter",
    y = "Rolling Average Sentiment Score"
  ) +
  theme_minimal(base_size = 15) +
  theme(
    legend.position = "none",
    plot.margin = margin(t = 10, r = 120, b = 10, l = 10), # more breathing space
    plot.title = element_text(face = "bold", size = 16),
    plot.subtitle = element_text(size = 12)
  ) +
  coord_cartesian(clip = "off", xlim = c(0, 40))

```


# 13. NAMED ENTITY RECOGNITION (NER) – EXTRACTING PLACES

# 13.1 INITIALIZE SPACY MODEL
```{r}

spacy_initialize(model = "en_core_web_sm")
```

# 13.2 PREPARE TEXT INPUT FOR NER
```{r}
texts <- setNames(book_chapters_clean3$text, book_chapters_clean3$chapter)
```

# 13.3 APPLY NER TO EXTRACT ENTITIES
```{r}

parsed_text <- spacy_parse(texts, entity = TRUE, nounphrase = FALSE)
entities    <- entity_extract(parsed_text)
```

# 13.4 FILTER FOR PLACE NAMES (GPE, LOC)
```{r}

# AFTER you build `places_data` (and before counting)
places_data <- entities %>%
  filter(entity_type %in% c("GPE", "LOC")) %>%
  mutate(
    chapter = as.integer(doc_id),
    entity  = stringr::str_squish(stringr::str_to_title(entity)),
    entity  = stringr::str_replace_all(entity, "[-–—_]", " ")
  ) %>%
  filter(!is.na(chapter), entity != "")

# Remove demonyms / non-locations
non_locations <- c("Parsee", "Parsi", "Englishman", "American", "Frenchman")
places_data <- places_data %>%
  filter(!entity %in% non_locations)

```

# 13.5 STANDARDIZATION
```{r}

# Standardize historical names like Bombay → Mumbai
map_tbl <- tibble::tibble(
  from = c("Hongkong", "Bombay", "Calcutta"),
  to   = c("Hong Kong", "Mumbai", "Kolkata")
)

places_data <- places_data %>%
  left_join(map_tbl, by = c("entity" = "from")) %>%
  mutate(entity = coalesce(to, entity)) %>%
  select(-to)
```

# 13.6 COUNT LOCATION FREQUENCIES
```{r}

place_counts <- places_data %>%
  count(entity, sort = TRUE) %>%
  mutate(rank = row_number())

place_by_chapter <- places_data %>%
  count(chapter, entity, name = "mentions") %>%
  arrange(chapter, desc(mentions))

# Preview
head(place_counts, 10)
```

# 13.7 VISUALIZE TOP 10 PLACES
```{r}

ggplot(place_counts %>% slice_head(n = 10),
       aes(x = reorder(entity, n), y = n)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(
    title = "Top 10 Locations Mentioned in 'Around the World in 80 Days'",
    subtitle = "Extracted using Named Entity Recognition (GPE and LOC)",
    x = "Location",
    y = "Number of Mentions"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 12)
  )
```

# 13.8 Visualise Journey Locations Across Chapters (Heatmap)
```{r}

# Order locations by Fogg's journey
journey_order <- c(
  "London",
  "England",
  "Mumbai",
  "India",
  "Hong Kong",
  "Shanghai",
  "Yokohama",
  "San francisco",
  "Omaha",
  "New york",
  "Liverpool",
  "America"  # broad US reference, appears around the American leg
)

# Filter and reorder factors properly
heatmap_df <- place_by_chapter %>%
  filter(entity %in% journey_order) %>%
  mutate(entity = factor(entity, levels = journey_order))

ggplot(heatmap_df, aes(x = chapter, y = entity, fill = mentions)) +
  geom_tile() + scale_y_discrete(limits = rev(journey_order)) +
  scale_fill_gradient(low = "white", high = "darkred") +
  labs(
    title = "Distribution of Journey Locations Across Chapters",
    x = "Chapter",
    y = "Location",
    fill = "Mentions"
  ) +
  theme_minimal(base_size = 12) +
  theme(plot.title = element_text(face = "bold", size = 14))

```

# 13.9 SAVE OUTPUTS 
```{r}

write.csv(place_counts,     "NER_Top_Locations.csv",       row.names = FALSE)
write.csv(place_by_chapter, "NER_Locations_By_Chapter.csv", row.names = FALSE)
```

# 13.10 FINALIZE SPACY SESSION

# Always finalize the spaCy session at the end to free system resources.

```{r}
spacy_finalize()
```

# 14. N-GRAM ANALYSIS (Bi-grams & Tri-grams)

# 14.1 Create n-grams (2-gram & 3-gram)
```{r}
clean_chapter_text <- book_chapters_clean3 %>%
  mutate(text = str_to_lower(text),
         text = str_replace_all(text, "[[:punct:]]", " "),
         text = str_replace_all(text, "\\s+", " "))
```

# 14.2 Generate Bi-grams
```{r}

bigrams <- clean_chapter_text %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

#Remove stopwords
bigrams_clean <- bigrams %>%
  separate(bigram, into = c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word) %>%
  unite(bigram, word1, word2, sep = " ") %>%
  count(bigram, sort = TRUE)
```

# 14.3 Generate Tri-grams
```{r}

trigrams <- clean_chapter_text %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3)

#Remove stopwords
trigrams_clean <- trigrams %>%
  separate(trigram, into = c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>%
  unite(trigram, word1, word2, word3, sep = " ") %>%
  count(trigram, sort = TRUE)
```

# 14.4 Top Bi-grams (Travel & Emotion Clues)
```{r}
top_bigrams <- bigrams_clean %>% slice_head(n = 15)

ggplot(top_bigrams,
       aes(x = n, y = reorder(bigram, n))) +
  geom_col(fill = "steelblue") +
  labs(
    title = "Top 15 Bi-grams in the Novel",
    x = "Frequency",
    y = "Bi-gram"
  ) +
  theme_minimal(base_size = 14)
```

# 14.5 Top Tri-grams (Story & Movement Patterns)
```{r}
top_trigrams <- trigrams_clean %>% slice_head(n = 15)

ggplot(top_trigrams,
       aes(x = n, y = reorder(trigram, n))) +
  geom_col(fill = "darkred") +
  labs(
    title = "Top 15 Tri-grams in the Novel",
    x = "Frequency",
    y = "Tri-gram"
  ) +
  theme_minimal(base_size = 14)
```

# 14.6 Save Outputs 
```{r}
write.csv(bigrams_clean, "bigrams_clean.csv", row.names = FALSE)
write.csv(trigrams_clean, "trigrams_clean.csv", row.names = FALSE)

```
